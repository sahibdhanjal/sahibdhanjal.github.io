<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Playfair+Display:ital,wght@0,400;0,700;1,400;1,700&display=swap">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0TL8C2VQKE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-0TL8C2VQKE');
  </script>
  <!-- End Google Tag -->

  <!--- basic page needs
      ================================================== -->
  <meta charset="utf-8">
  <title>Sahib Dhanjal | Robotics & AI Engineer</title>
  <meta name="author" content="Sahib Dhanjal">
  <meta name="description" content="Sahib Dhanjal - Robotics & AI Engineer Portfolio. Expert in Computer Vision, Deep Learning, SLAM, and Autonomous Systems. Explore my projects and blog.">
  <meta name="keywords" content="sahib dhanjal, sahib singh, blog, portfolio, perception, autonomy, engineer, deep learning, slam, vio, machine learning, robotics, computer vision, lidar, camera, online calibration, calibration, odometry, IMU, GPS, WiFI, google, AR/VR, augmented reality, virtual reality, XR, extended reality, self-driving cars, autonomous vehicles, autonomous driving, SAE level 5, magic leap, lyft level 5, magna, nasa, astronet, dasc lab, tardec, ros, gazebo, matlab, oculus rift, crazyflie, ardupilot, vicon, cuda, localization, mapping, sensor fusion, swarm robotics, indoor positioning, wifi localization, deeplocnet, unsupervised learning, gmm, em, posenet++, structure from motion, sfm, gtsam, isam, mask r-cnn, yolo, pedestrian tracking, particle filter, path planning, rrt, kinematics, robot arm, arduino, raspberry pi, beaglebone black, solidworks, kinect, formula sae, vehicle dynamics, control theory, autonomous maritime systems, robotx, auvsi, ai, artificial intelligence, expert, tutorial, python, c++, pytorch, LLM, large language model, vision transformer, segmentation, instance segmentation, object tracking, object detection, optical flow">
  <!-- mobile specific metas
      ================================================== -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- CSS
      ================================================== -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
    integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
  <link rel="stylesheet" href="css/style.min.css">
  <!-- favicons
      ================================================== -->
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="images/favicon.ico" type="image/x-icon">
</head>

<body id="top">
  <!-- header
      ================================================== -->
  <header class="s-header">
    <div class="header-logo">
      <a class="site-logo" href="#home">
        <h1 style="font-family: sans-serif; font-size: 3rem; color: white; font-family: domine-bold, sans-serif;">
          Sahib Dhanjal
        </h1>
      </a>
    </div>
    <div class="nav-mobile">
      <a id="nav-toggle" href="#!"><span></span></a>
    </div>
    <nav class="header-nav">
      <ul class="header-nav__list">
        <li class="nav-home current"><a href="#home">Home</a></li>
        <li><a href="#work">Work</a></li>
        <li><a href="#projects">Projects</a></li>
        <li><a href="#blog">Blog</a></li>
        <li><a href="#about">About</a></li>
        <li><a href="#resume" id="resume-link">Resume</a></li>
      </ul>
    </nav>
  </header>
  <!-- home
      ================================================== -->
  <section id="home" class="s-home target-section" data-parallax="scroll" data-position-y=top>
    <canvas id="cover-canvas"></canvas>
    <div class="cover-content">
      <h1 class="cover-title">
        I help robots <br>
        see the <span class="serif-italic">real world.</span>
      </h1>
      <div class="cover-description">
        <p>Computer Vision ‚Ä¢ Deep Learning ‚Ä¢ SLAM </p>
        <p>Based in San Francisco.</p>
      </div>
    </div>
    <a href="#work" class="scroll-down-arrow"><i class="fa fa-chevron-down"></i></a>
  </section>
  <!-- end s-home -->
  <!-- work
      ================================================== -->
  <section id="work" class="s-work target-section">
    <div class="section-label">
      <span class="dot"></span> WORK
    </div>
    <div class="work-experience" data-aos="fade-up">
      <!-- Magic Leap / Google -->
      <div class="experience-item">
        <div class="experience-company">Magic Leap | Google</div>
        <div class="experience-details">
          <div class="experience-header">
            <h3 class="experience-title">Perception Engineer</h3>
            <span class="experience-date">Oct '20 - Present</span>
          </div>
          <p class="experience-desc">Designing, implementing and scaling state-of-the-art algorithms across the
            perception stack, enabling seamless XR experiences under the <a
              href="https://www.magicleap.com/newsroom/magic-leap-and-google-partnership">Magic Leap - Google
              Partnership</a>.
          </p>
        </div>
      </div>
      <!-- Magna / Lyft Level 5 -->
      <div class="experience-item">
        <div class="experience-company">Magna | Lyft Level 5</div>
        <div class="experience-details">
          <div class="experience-header">
            <h3 class="experience-title">Localization & Mapping Engineer</h3>
            <span class="experience-date">Jul '19 - Oct '20</span>
          </div>
          <p class="experience-desc">Focused on deploying detection and localization pipelines on highly constrained
            hardware, while also developing the next generation of multi-layered HD maps under the <a
              href="https://www.magna.com/company/newsroom/releases-archive/release/2019/01/14/news-release---magna-highlights-lyft-partnership-milestones-at-2019-north-american-international-auto-show">Magna
              - Lyft Level 5 Partnership</a>.
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- end s-work -->
  <!-- projects
      ================================================== -->
  <section id='projects' class="s-projects target-section">
    <div class="section-label" data-aos="fade-up">
      <span class="dot"></span> PROJECTS
    </div>
    <div class="projects-grid">
      <!-- NASA Astronet -->
      <div class="project-card" data-aos="fade-up" data-date="Apr '18 - May '19">
        <div class="project-card__img"><img src="images/portfolio/nasa.jpg" alt="NASA Astronet"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">NASA Astronet</h3>
          <p class="project-card__cat">A Human-Centric Network of Co-Robots</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            As a part of the <a href="https://dasc-lab.github.io/">DASC Lab</a>, over the course of 1.5 years, I
            implemented and scaled multiple optimization-based control and coverage algorithms developed by <a
              href="https://scholar.google.com/citations?user=e4efxWQAAAAJ&hl=en">Will</a> and
            <a href="https://scholar.google.com/citations?user=ny1yTusAAAAJ&hl=en">Dr. Dimitra Panagou</a>,
            translating theoretical guarantees into a deployable and simulation-validated system. <a
              href="https://www.nasa.gov/directorates/spacetech/strg/ecf2016/AstroNet.html">AstroNet</a> was one such
            research initiative focused on enabling autonomous coverage and discrete navigation strategies for NASA‚Äôs
            free-flying <a href="https://www.nasa.gov/astrobee">Astrobee</a> robots operating aboard the ISS, with the
            goal of reducing reliance on human teleoperation.
          </p>
          <!-- Embed Video here instead of image -->
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/hkFtmTng5Wo?si=HYrI0QJOKNnxbLEI" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
          <p>
            As can be seen in the embedded video, in this project I primarily focused on building a high-fidelity ISS
            simulation environment using <a href="http://wiki.ros.org/">ROS</a> and <a
              href="https://gazebosim.org/home">Gazebo</a> to evaluate coverage strategies under realistic microgravity
            dynamics and sensing constraints. This included developing an optimized real-time control pipeline capable
            of executing discrete coverage policies within the ROS ecosystem while maintaining stable closed-loop
            behavior.
          </p>
          <p>
            To support end-to-end evaluation, I designed a distributed architecture integrating MATLAB (core control and
            coverage algorithms), ROS (middleware), Gazebo (physics simulation), and an immersive VR visualization
            interface via an <a href="https://www.oculus.com/rift/">Oculus Rift</a>. This framework enabled closed-loop
            validation of guidance, navigation, and control (GNC) strategies prior to hardware deployment, significantly
            improving experimental throughput and reproducibility.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/NASA-Astrobee" target="_blank">Github</a>
          </div>
        </div>
      </div>
      <!-- Swarm Mapping -->
      <div class="project-card" data-aos="fade-up" data-date="Dec '18 - May '19">
        <div class="project-card__img"><img src="images/portfolio/swarm.jpg" alt="Swarm Mapping"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Swarm Mapping and Control</h3>
          <p class="project-card__cat">Autonomous Aerial and Ground Robot Swarm</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>This was one of the more interesting robotics projects I worked on, in collaboration with <a
              href="https://tardec.army.mil/">TARDEC</a>, and guided by <a
              href="http://www-personal.umich.edu/~dpanagou/">Dr. Dimitra Panagou</a>, where I used a mix of <a
              href="https://www.bitcraze.io/crazyflie-2/">CrazyFlie drones</a> and <a
              href="https://store.aionrobotics.com/products/r1-ardupilot">Aion R1 rovers</a> for autonomous
            reconnaissance missions using multiple robots. The goal was to get a swarm of heterogeneous robots to work
            together seamlessly‚Äîeven in unpredictable environments‚Äîwhile avoiding collisions and keeping track of their
            positions.
          </p>
          <!-- Embed Video here instead of CrazyFly Image -->
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/TLIx-w4eX3c?si=K54d36zy_Adxo5EI" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
          <p>
            My main focus was on multi-robot localization and collision avoidance, helping the drones and rovers
            navigate in sync without stepping on each other‚Äôs toes. I was also solely responsible for the system
            architecture as well, where I developed a pub-sub architecture using ROS for this swarm of robots to
            communicate with each other.The project gave me hands-on experience with distributed algorithms, sensor
            fusion, and resilient swarm coordination. As you can see from the videos attached, we were trying to make
            the robots follow generic routines such as following a set path or creating a defined shape while the
            control algorithms were running in real-time. From the embedded videos, you can see how a set of 4 drones,
            as well as 3 rovers were able to create basic shapes without colliding or running into each other. For the
            purposes of the shown experiments, localization was done using simple vicon motion-capture systems in indoor
            environments but using SLAM outdoors using the sensors (lidar + odometry) the rovers were equipped with.
          </p>
          <!-- Embed Video here instead of Rover Image -->
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/EFuOMRC9ebY?si=9T9KmCBpLbrL5d_J" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!-- Visual-Radio-Inertial Localization -->
      <div class="project-card" data-aos="fade-up" data-date="Aug '18 - May '19">
        <div class="project-card__img"><img src="images/portfolio/wifi.jpg" alt="Radio Localization"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Visual-Radio-Inertial Localization</h3>
          <p class="project-card__cat">WiFi based Indoor Positioning System</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This project was more attuned to be a masters theses than anything. From the time I learned about SLAM, I
            wanted to see if robots/devices can locate inside indoor environments using signals from low cost sensors
            such as WiFi/BLE Beacons which are already installed in most buildings. Working under <a
              href="https://scholar.google.com/citations?user=l2jdSb8AAAAJ&hl=en">Dr. Maani Ghaffari</a>, I was working
            on exploring how radio waves, specifically WiFi can be used to accurately locate the 3DOF position of a
            device inside a GPS denied environment. After much exploration, I was able to create a deep neural network
            which was able to differentiate between Line-Of-Sight/Non-Line-Of-Sight packets with an accuracy of ~95%.
            Without this classifier, the robot often lost track of its position in the environment as seen in the
            illustration below (FastSLAM without classifier).
          </p>
          <img class="project-card__gallery-img"
            src="https://github.com/sahibdhanjal/DeepLocNet/raw/matlab/assets/FSwoC.gif" alt="Radio Localization">
          <p>
            The algorithm was initially developed and tested using the open-source <a
              href="https://pylayers.github.io/pylayers/">PyLayers</a> simulator. On confirmation that our core
            algorithm worked, we proceeded to test it out on the <a href="https://fetchrobotics.com/">Fetch</a> robot,
            using the Friis Free Space model and our classifier to act as a measurment in Fast SLAM, while using the
            robot's wheel odometry as its positioning model. A variant using particle filter based SLAM was also
            implemented to evaluate performance. In our experiments, we were able to achieve a remarkably
            accurate localization with a ~1.6m RMSE in 35m x 170m indoor environment. We can see how the FastSLAM
            performs when the classifier is used to inform its state in the animation below.
          </p>
          <img class="project-card__gallery-img"
            src="https://github.com/sahibdhanjal/DeepLocNet/raw/matlab/assets/FSwC.gif" alt="Radio Localization">
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/DeepLocNet" target="_blank">Github</a>
            <a href="https://arxiv.org/abs/2002.00484" target="_blank">Paper</a>
          </div>
        </div>
      </div>
      <!-- Unsupervised Learning AR -->
      <div class="project-card" data-aos="fade-up" data-date="Jul '18 - Sep '18">
        <div class="project-card__img"><img src="images/portfolio/arcontrol.jpg" alt="AR Control"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">AR Based Task Tracking Drone</h3>
          <p class="project-card__cat">Unsupervised Learning based on GMM</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            Another project where I was working with <a
              href="https://scholar.google.com/citations?user=e4efxWQAAAAJ&hl=en">Will</a> and
            <a href="https://scholar.google.com/citations?user=ny1yTusAAAAJ&hl=en">Dr. Dimitra Panagou</a> where we were
            using an AR glass, as well as a camera to provide the robot with assistive camera views. The vicon
            motion capture system was also used to track ground-truth head motions, which was then used to fit a
            Gaussian Mixture Model (GMM) to define the visual interest function of the user using online Expectation
            Maximization (EM). My role on this project was solely enablement - I was responsible for setting up the
            entire software stack for the project, including the AR glasses, the camera, and the vicon system as well as
            implementing the algorithm in C++ as well as optimizing the hell out of it using CUDA.
          </p>
          <!-- Embed Video here instead of Image -->
          <div class="video-container">
            <iframe src="https://player.vimeo.com/video/297490185?h=6a0638e0bf" width="640" height="360" frameborder="0"
              allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
          <p>
            You can see in the embedded video how the drone is autonomously able to navigate to the task which is of
            highest importance, but not being currently focused on by the operator. The core goal of this project was
            that lets say an operator has multiple tasks to do - task 1, task 2, task 3 (ordered by priority), if the
            operator is focusing on task 2 at the moment, the drone automatically navigates to task 1 while providing a
            visual feed on how that is unfolding. For in-depth details on this project, I'd highly encourage you to
            refer to the paper we published with are findings in it.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/AR-unsupervised-learning" target="_blank">Github</a>
            <a href="https://ieeexplore.ieee.org/document/8793587" target="_blank">Paper</a>
          </div>
        </div>
      </div>
      <!-- Balance Bot -->
      <div class="project-card" data-aos="fade-up" data-date="Sep '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/balance.jpg" alt="Balance Bot"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Autonomous Balance Bot</h3>
          <p class="project-card__cat">Self Balancing Two-Wheeled Bot</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            Another one of those fun projects from the <a href="https://rob550-docs.github.io/">UMich Curriculum</a>,
            in this project, a team of 2 people was responsible for building a self-balancing two-wheeled bot from
            scratch. We had to design everything from scratch ‚Äî all the way from mechanical design, to the cascaded PID
            controller. The goal of the project was to autonomously navigate obstacle courses while maintaining its
            balance and positioning.
          </p>
          <!-- Embed Video here instead of image -->
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/HnjD2pixu-w?si=T4j4UxFzgoyLtDCH" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
          <p>
            We used a combination of wheel odometry and Vicon based motion capture to estimate the bot's current
            position and orientation. Control algorithms were developed to control the bot's balance, motion and
            orientation, as well as dictating what it needs to do. The bot was able to navigate obstacle courses while
            maintaining its balance and positioning, that too within very strict time limits. In a competition of 12
            teams, ours was the fastest and most stable!
          </p>
        </div>
      </div>
      <!-- PoseNet++ -->
      <div class="project-card" data-aos="fade-up" data-date="Feb '18 - Apr '18">
        <div class="project-card__img"><img src="images/portfolio/posenet.jpg" alt="PoseNet++"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">PoseNet++</h3>
          <p class="project-card__cat">Deep Learning for SLAM</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>In this project, I developed an online deep-learning framework for automatic data labeling of mobile camera
            input using structure-from-motion (SfM). A deep learning‚Äìbased structural motion algorithm generated
            high-quality pose labels from raw images, eliminating the need for manual annotation. These labeled poses
            were used to train PoseNet as a visual sensor model, while GPS and odometry data provided the action model
            for the robot. The approach integrated learning-based perception with probabilistic state estimation in a
            factor graph using <a href="https://borg.cc.gatech.edu/">GTSAM</a> and <a
              href="http://people.csail.mit.edu/kaess/isam/">iSAM</a>.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-posenet.jpg" alt="PoseNet++">
          <p><a
              href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf">PoseNet</a>
            served as the learned observation model, regressing camera poses from images, which were fused with
            motion priors from odometry and GPS in real time. The system was first validated in GTSAM simulations to
            ensure consistency and robustness before deployment on a differential drive mobile robot. In real-world
            operation, camera inputs were processed online, and PoseNet outputs were integrated with motion data to
            provide continuously refined pose estimates.
          </p>
          <p>By combining deep learning with classical state estimation, this framework offered a scalable, hybrid
            approach for robust localization. It demonstrated that automatically labeled camera data could train
            effective sensor models, while probabilistic factor-graph optimization ensured globally consistent pose
            estimation for mobile robotic systems.
          </p>
          <div class="project-card__links">
            <a href="https://posenet-mobile-robot.github.io/" target="_blank">Website</a>
            <a href="https://pdfs.semanticscholar.org/e389/cbc946b7e6a9baab781f8a2cdb3deea356dd.pdf"
              target="_blank">Paper</a>
          </div>
        </div>
      </div>
      <!-- 3D Bounding Box -->
      <div class="project-card" data-aos="fade-up" data-date="Oct '18 - Dec '18">
        <div class="project-card__img"><img src="images/portfolio/gta.jpg" alt="3D Bounding Box"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">3D Bounding Box Regression</h3>
          <p class="project-card__cat">Geometric Deep Learning</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            A more trivial project (compared to the previous ones ofcourse), this one was a part of our curriculum at
            UMich as well. We were given the GTA 10k dataset and were supposed to develop a custom neural network to
            regress the 3D bounding box of vehicles in the dataset using a simplistic bounding box detector and
            geometric constraints. As a solution, I developed a simplistic 20-layer SE-ResNet + YOLO 3D bounding box
            regression incorporating ideas from <a href="https://arxiv.org/abs/1612.00496">Mousavian et al.</a>. It was
            a Kaggle competition, where I ranked 5th out of 41 teams, achieving an accuracy of ~73% with the centroid
            MSE of ~9.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-gta.jpg" alt="3D Bounding Box">
        </div>
      </div>
      <!-- Pedestrian Tracking -->
      <div class="project-card" data-aos="fade-up" data-date="Feb '18 - Apr '18">
        <div class="project-card__img"><img src="images/portfolio/ped.jpg" alt="Pedestrian Tracking"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Pedestrian Tracking</h3>
          <p class="project-card__cat">Mask R-CNN based Pedestrian Tracking</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            Pedestrian tracking is a long standing problem and multiple ways of tracking them accurately have already
            been developed. The problem comes in when pedestrians are occluded or when there are multiple pedestrians
            in the scene. In this project, I developed a pedestrian detection and tracking system that combines
            deep learning‚Äìbased instance segmentation with probabilistic multi-target tracking using a particle filter.
            A pre-trained <a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN</a> was modified to only capture
            and segment pedestrians at the pixel level allowing for precise localization even in partially occluded or
            crowded scenes.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-ped.jpg" alt="Pedestrian Tracking">
          <p>
            We integrated a <a
              href="https://web.stanford.edu/~blange/data/Probability%20Hypothesis%20Density%20Filter%20Implementation%20and%20Application.pdf">Probability
              Hypothesis Density (PHD) filter</a> with dense optical flow, and image segmentation cues to enable
            reliable multi-target tracking without requiring explicit data association for every object, while optical
            flow captured short-term motion dynamics between consecutive frames. By fusing motion information with
            segmentation outputs, the system was able to estimate and update the trajectories of multiple pedestrians
            simultaneously, even when they crossed paths or experienced temporary occlusion. The pipeline was evaluated
            on the <a href="https://www.cityscapes-dataset.com/">Cityscapes</a> and <a
              href="https://www.kaggle.com/datasets/solesensei/solesensei_bdd100k">Berkeley BDD100K</a> datasets, where
            it demonstrated strong performance in low to medium-cluttered urban scenes.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/Mask-RCNN-Pedestrian-Detection" target="_blank">Github</a>
          </div>
        </div>
      </div>
      <!-- AUVSI RobotX -->
      <div class="project-card" data-aos="fade-up" data-date="Jan '18 - Oct '18">
        <div class="project-card__img"><img src="images/portfolio/wamv.jpg" alt="AUVSI RobotX"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">AUVSI RobotX</h3>
          <p class="project-card__cat">Autonomous Maritime Systems</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            The <a href="http://robotx.org/">AUVSI RobotX</a> Challenge is an international robotics competition
            focused on designing fully autonomous maritime systems capable of navigating complex marine environments
            without human intervention. For this project, I developed a robust software and perception stack to support
            high-fidelity autonomy on an unmanned surface vehicle (USV). I architected and implemented a scalable
            sensor-fusion framework that tightly integrates a spatial dual GPS/IMU module with high-resolution imaging
            from the FLIR Ladybug 3 and dense point-cloud data from a Velodyne HDL-32E, enabling accurate state
            estimation and situational awareness in real time.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-wamv.jpg" alt="AUVSI RobotX">
          <p>
            To meet competition requirements, I implemented classical robotic control algorithms alongside a
            YOLO-based marker detection system for visual identification of competition objects and task elements,
            improving the vehicle‚Äôs ability to localize and interact with its environment. I also developed advanced
            calibration pipelines for LiDAR‚ÄìLiDAR and camera‚ÄìLiDAR extrinsic alignment using PnP and 3D correspondences,
            based on established multi-sensor calibration techniques, which significantly enhanced the coherence of
            multi-modal data streams for perception and planning. Finally, I deployed a SLAM framework in ROS leveraging
            gmapping, costmap_2d, and AMCL libraries, enabling reliable mapping, localization, and autonomous waypoint
            navigation within the competition arena and demonstrating an end-to-end autonomy solution capable of
            perception, mapping, and decision-making under real-world constraints.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/MichiganRobotX/" target="_blank">Github</a>
          </div>
        </div>
      </div>
      <!-- SLAM -->
      <div class="project-card" data-aos="fade-up" data-date="Oct '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/slam.jpg" alt="SLAM"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Simultaneous Localization & Mapping</h3>
          <p class="project-card__cat">Particle Filter based SLAM</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This was another one of those amazing projects which the curriculum at Michigan had to offer. We were
            given a simple differential drive mobile robot with all the drivers already implemented. The only thing
            we had to implement was particle filter based SLAM on it using a <a
              href="https://www.servomagazine.com/magazine/article/the-multi-rotor-hobbyist-scanse-sweep-3d-scanner-review">Scanse
              Sweep Lidar</a>. To enable this, I implemented the occupancy grid map as well as the particle filter
            localization algorithm on a <a href="https://www.raspberrypi.org/">Raspberry Pi 3</a>.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-slam.jpg" alt="SLAM">
          <p>
            We also implemented <a href="http://robotfrontier.com/">Yamauchi's exploration</a> method on the secondary
            computer, a <a href="https://beagleboard.org/black">BeagleBone Black</a>. This enabled the robot to freely
            explore and map out an entire room without any manual intervention.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/SLAM/blob/master/BotLab%20Report.pdf" target="_blank">Github</a>
          </div>
        </div>
      </div>
      <!-- Path Planning -->
      <div class="project-card" data-aos="fade-up" data-date="Dec '15 - May '16">
        <div class="project-card__img"><img src="images/portfolio/turt.jpg" alt="Path Planning"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Path Planning | Exploration</h3>
          <p class="project-card__cat">Multi-Agent Exploration using TurtleBots</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This project focused on the development and large-scale evaluation of autonomous path planning and
            multi-robot exploration algorithms using <a href="https://www.turtlebot.com/turtlebot2/">Turtlebot</a>
            platforms within the <a href="www.ros.org/">ROS</a> ecosystem. I designed and implemented a complete
            navigation stack, including global and local planning components, enabling reliable autonomous exploration
            and waypoint navigation across structured and unknown environments.
          </p>
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/NOwfoaqi4M8?si=1aX3V4VqMFkeblO-" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
          <p>To support algorithmic evaluation at scale, I also developed a Python-based multi-robot simulator for
            autonomous exploration and path planning, enabling rapid prototyping and benchmarking of multi-agent
            coordination strategies. The project, sponsored by the Defence Research and Development Organisation(DRDO),
            involved high-fidelity simulation of multi-robot exploration in Gazebo, bridging theoretical planning
            methods with realistic robotic constraints.
          </p>
          <p>I conducted a comparative analysis of classical and multi-agent exploration algorithms across more than 50
            grid maps (50√ó50 resolution) and validated performance experimentally using TurtleBots across 10 physical
            and simulated environments. The work provided quantitative insights into exploration efficiency,
            scalability, coordination overhead, and convergence behavior, contributing to a reproducible evaluation
            framework for decentralized and cooperative robotic navigation strategies.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/Path-Planning-Simulator" target="_blank">Github</a>
            <a href="projects/path planning simulator/index.html" target="_blank">Demo</a>
          </div>
        </div>
      </div>
      <!-- Kinematics Simulator -->
      <div class="project-card" data-aos="fade-up" data-date="Sep '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/kinsim.jpg" alt="Kinematics Simulator"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Robot Kinematics Simulator</h3>
          <p class="project-card__cat">Javascript based Kinematic Simulator</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This project was part of one of the courses I'd taken - <a href="https://autorob.github.io/">Autonomous
              Robotics</a>. As a part of this project, I developed a web-based simulator which was capable of
            interfacing with ROS and actually control a Fetch Robot. The project involved me implementing parsers for
            URDFs to create robots models in 3D, forward/inverse kinematics, object following, as well as trajectory
            planning algorithms based on Rapidly-Exploring Random Trees (RRT). You should definitely checkout the Demo
            to play around with the 3D model and see it in action!
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-kinsim.jpg" alt="Kinematics Simulator">
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/3D-RobotSimulator" target="_blank">Github</a>
            <a href="projects/kinematic simulator/home.html" target="_blank">Demo</a>
          </div>
        </div>
      </div>
      <!-- Formula SAE -->
      <div class="project-card" data-aos="fade-up" data-date="Oct '12 - Dec '14">
        <div class="project-card__img"><img src="images/portfolio/fs.jpg" alt="Formula SAE"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Formula SAE üáÆüáπ</h3>
          <p class="project-card__cat">Vehicle Dynamics for Sports Cars</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            One of my favorite projects at BITS, Pilani - I was a part of the Formula SAE team where I led the
            design and development of the physics of a double wishbone push-rod suspension, made multiple weight
            optimizations to the bell-crank as well as optimized the suspension dynamics to enable maximal cornering
            control at high speeds and capable of bearing upto 2g in lateral force. I was fortunate enough to
            represent our team at <a href="https://www.sae.org/attend/student-events/">FSAE Italy '14</a>, where we
            ranked 4th / 46 international teams in design. Unfortunately, our vehicle was broken during transportation
            so we couldn't participate in any of the dynamic events. Since I was also a part of the marketing and
            sponsorship team, I also created multiple marketing materials and brought in multiple investors for our
            initiative. You'd get an idea of how passionate we were about this project, just look at the video!
          </p>
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/A1FeSceSzeU?si=2MmwVWs7AoE6mhMq" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
          <div class="project-card__links">
            <a href="https://www.facebook.com/inspiredkarters/" target="_blank">Link</a>
          </div>
        </div>
      </div>
      <!-- Pendulum Simulations -->
      <div class="project-card" data-aos="fade-up" data-date="Sep '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/pend.jpg" alt="Pendulum Simulations"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Game Physics Simulations</h3>
          <p class="project-card__cat">Single, Double and Cart Pole Pendulums</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This project was a primer to the kinematics simulator, a part of the course - <a
              href="https://autorob.github.io/">Autonomous
              Robotics</a>. In this project, I developed multiple web-based simulators for single, double and cart-poles
            to test out control algorithms. Some of the control algorithms I implemented were PID control based on:
            Euler, Verlet, Velocity Verlet and Runge-Kutta 4th order integration methods. This project was basically my
            primary exposure to control theory and numerical methods, setting me up to work on significantly complex
            projects across autonomous robotics and manipulators. Definitely check out the linked demos to see the
            control algorithms in action! These one-pager websites primarily work using JavaScript and HTML5 Canvas.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-pend.jpg" alt="Pendulum Simulations">
          <div class="project-card__links">
            <a href="projects/pendulum/pendularm1.html" target="_blank">Single Pendulum</a>
            <a href="projects/pendulum/pendularm2.html" target="_blank">Double Pendulum</a>
            <a href="projects/pendulum/cartpole.html" target="_blank">Cart Pole</a>
          </div>
        </div>
      </div>
      <!-- Sense -->
      <div class="project-card" data-aos="fade-up" data-date="May '19 - Jul '19">
        <div class="project-card__img"><img src="images/portfolio/sense.jpg" alt="Sense"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Sense</h3>
          <p class="project-card__cat">App Streaming Sensor Data via WebSockets</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This was one of my side projects where I worked on repurposing my old Android phone as a mobile sensor
            platform, streaming its built-in sensors‚ÄîGPS, IMU, camera, and more‚Äîover a websocket to a web server. It was
            an attempt to reuse old hardware and start experimenting with robotics, SLAM algorithms, or real-time sensor
            data without needing expensive equipment which I was generally used to.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-sense.jpg" alt="Sense">
          <p>
            Alongside the app, I also created a <a href="www.ros.org/">ROS package</a> making it easier to subscribe
            to sensor topics and integrate the phone‚Äôs data into the SLAM algorithms I was playing around with. I also
            intentionally made it modular so as to include more sensors / custom processing in the future.
          </p>
        </div>
      </div>
      <!-- Vision Robotic Arm -->
      <div class="project-card" data-aos="fade-up" data-date="Sep '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/aarm.jpg" alt="Autonomous Arm"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Vision Based Autonomous Arm</h3>
          <p class="project-card__cat">Microsoft Kinect + Robot Arm</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            Another project as part of my robotics curriculum, in this project we were tasked with designing and
            programming a robotic arm capable of detecting colored cubes, picking them up and stacking them in a
            specific order. As part of a three-person team, I designed the mechanical structure for the RRR:R dynamixel
            arm in SolidWorks, 3D printed it, and assembled it with the motors and electronics. The arm was controlled
            using a Raspberry Pi and a Python script was implemented which interfaced with the Dynamixel Motors and
            <a href="https://www.xbox.com/en-US/xbox-one/accessories/kinect">Microsoft
              Kinect</a> to detect colored cubes via depth maps and plan trajectories using splines. I implemented the
            forward/inverse kinematics for this arm, whereas my team mates worked on block detection and trajectory
            planning.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-aarm.jpg" alt="Autonomous Arm">
        </div>
      </div>
      <!-- Gesture Arm -->
      <div class="project-card" data-aos="fade-up" data-date="Oct '14 - Dec '14">
        <div class="project-card__img"><img src="images/portfolio/garm.jpg" alt="Gesture Arm"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Gesture Controlled Robotic Arm</h3>
          <p class="project-card__cat">Arduino and XBee Based Robot Arm</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            Another one of my undergrad projects, where I worked on designing and developing a gesture controlled
            robotic arm under the guidance of <a
              href="https://scholar.google.co.in/citations?user=rwWOPY8AAAAJ&hl=en">Dr.
              R.K.Mittal</a>. I used two MPU 6050s, fixed on a glove to estimate the motion of the user's hand. This
            was coupled with XBees and an Arduino UNO for wireless commnuication with the main laptop, which basically
            enabled the robotic arm to mimic the users hand gestures. The robotic arm was large enough to completely
            span a 2ft hemispherical workspace, with a 500g payload.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-garm.jpg" alt="Gesture Arm">
        </div>
      </div>
      <!-- White Board Cleaner -->
      <div class="project-card" data-aos="fade-up" data-date="May '15 - Jun '15">
        <div class="project-card__img"><img src="images/portfolio/warm.jpg" alt="Whiteboard Cleaner"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Portable WhiteBoard Cleaner</h3>
          <p class="project-card__cat">Arduino-Powered Manipulator</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This was my very first introduction to robotics so it really is close to my heart. In this summer
            internship, I worked with <a href="https://scholar.google.com/citations?user=gW4Q_7UAAAAJ&hl=en">Dr. D.K.
              Pratihar</a> from IIT Kharagpur, designing and developing a portable whiteboard cleaner (RR:PR
            manipulator) for white/black boards up to 4'√ó6'. I worked on chip design, fabrication as well as programming
            the servo motors to automatically clean the entire board. I also presented a whitepaper of this work
            at <a href="https://upcon15.iiita.ac.in/">IEEE UPCON'15</a>.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-warm.jpg" alt="Whiteboard Cleaner">
          <div class="project-card__links">
            <a href="https://ieeexplore.ieee.org/abstract/document/7456702" target="_blank">Paper</a>
          </div>
        </div>
      </div>
    </div>
    <!-- end projects-grid -->
    <div id="projects-pagination"></div>
    <!-- Project Detail Overlay -->
    <div id="project-overlay" class="project-overlay">
      <div class="project-overlay__content">
        <button class="project-overlay__close" aria-label="Close">&times;</button>
        <div class="project-overlay__header">
          <div class="project-overlay__left">
            <h2 class="project-overlay__title"></h2>
            <div class="project-overlay__desc"></div>
            <div class="project-overlay__img-wrap">
              <img class="project-overlay__img" src="" alt="">
            </div>
          </div>
          <div class="project-overlay__right">
            <div class="project-overlay__meta">
              <div class="project-overlay__meta-row">
                <span class="project-overlay__meta-label">Date:</span>
                <span class="project-overlay__meta-value project-overlay__date"></span>
              </div>
              <div class="project-overlay__meta-row" id="proj-link-row">
                <span class="project-overlay__meta-label">Link:</span>
                <span class="project-overlay__meta-value project-overlay__links"></span>
              </div>
            </div>
            <div class="project-overlay__nav">
              <a href="javascript:void(0)" class="project-overlay__nav-btn" id="overlay-prev">&larr; Prev</a>
              <a href="javascript:void(0)" class="project-overlay__nav-btn" id="overlay-next">Next &rarr;</a>
            </div>
          </div>
        </div>
      </div>
  </section>
  <!-- end s-projects -->
  <!-- blog
      ================================================== -->
  <section id="blog" class="s-blog target-section">
    <div class="section-label" data-aos="fade-up">
      <span class="dot"></span> BLOG
    </div>
    <div class="blog-container" data-aos="fade-up">
      <p class="blog-section-sub">I also run a blog where I share interesting things I learn at work, offer tips for
        beginners, and write about tech / trading topics that others might find helpful. If you love what you see,
        definitely check out my <a href="https://sahibdhanjal.medium.com/" target="_blank" rel="noopener">entire archive
          on Medium</a> !
      </p>
      <div id="blog-list">
        <!-- Blog items injected here by js/blog.js -->
        <p style="color:rgba(255,255,255,0.4);">Loading posts‚Ä¶</p>
      </div>
      <div id="blog-pagination"></div>
    </div>
  </section>
  <!-- end s-blog -->

  <!-- about me
      ================================================== -->
  <section id="about">
    <div class="about-content">
      <div class="about-left" data-aos="fade-up">
        <div class="section-label" data-aos="fade-up">
          <span class="dot"></span> ABOUT ME
        </div>
        <div class="about-desc">
          <p>Hey! üëã</p>
          <p>I‚Äôm <strong>Sahib Dhanjal</strong>, a robotics engineer obsessed with building machines that can
            think, move, and navigate the world on their own. I spend my days (and many late nights) tinkering with
            hardware, experimenting with algorithms, and pushing robots to better understand and interact with their
            surroundings.</p>
          <p>Outside work, you‚Äôll find me playing soccer, ultimate frisbee, or volleyball ‚Äî maybe even running üèÉ,
            hiking üèîÔ∏è, and biking üö¥üèª.</p>
          <p>Always <span id="changing-text"></span></p>
        </div>
        <div class="about-buttons">
          <a href="mailto:dhanjalsahib@gmail.com" class="btn-pill btn-primary">Contact Me</a>
          <a href="https://www.linkedin.com/in/sahibdhanjal/" target="_blank"
            class="btn-pill btn-secondary">LinkedIn<img
              src="https://cdn.prod.website-files.com/63dcb6e1a80e9454b630f4c4/63e0b50ea0956f4526968ef1_23-icon-external.svg"
              loading="lazy" alt="" class="icon-external" style="margin-left:8px; vertical-align: middle;"></a>
        </div>
      </div>
      <div class="about-right" data-aos="fade-up">
        <div class="profile-img-container">
          <img src="images/me.jpg" alt="Sahib Dhanjal" class="profile-img">
        </div>
      </div>
    </div>
  </section>
  <!-- end s-about -->
  <!-- contact
      ================================================== -->
  <section id="contact" class="s-contact">
    <div class="col-full cl-copyright" style="margin-bottom: 1rem; margin-top: 1rem;">
      <script>document.write(new Date().getFullYear());</script> | <a href="https://sahibdhanjal.github.io/">Sahib
        Dhanjal</a>
      <ul class="contact-social">
        <li><a href="https://www.linkedin.com/in/sahibdhanjal/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a href="https://github.com/sahibdhanjal/" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a href="https://scholar.google.com/citations?user=sYbUGw8AAAAJ" target="_blank"><i
              class="fa fa-graduation-cap"></i></a></li>
        <li><a href="https://medium.com/@sahibdhanjal" target="_blank"><i class="fa fa-medium"></i></a></li>
      </ul>
    </div>
  </section>
  <!-- end s-contact -->
  <!-- Java Script
      ================================================== -->
  <script src="js/scripts.min.js"></script>

  <!-- Resume Overlay -->
  <div id="resume-overlay">
    <span class="close-btn">&times;</span>
    <div class="resume-container">
      <iframe src="resume/Sahib Dhanjal Resume.pdf#toolbar=1&navpanes=0&view=FitW&frameborder=0"
        title="Resume"></iframe>
    </div>
  </div>
</body>

</html>