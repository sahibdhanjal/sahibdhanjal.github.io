<!DOCTYPE html>
<html class="no-js" lang="en">

<head>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0TL8C2VQKE"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-0TL8C2VQKE');
  </script>
  <!-- End Google Tag -->

  <!--- basic page needs
    ================================================== -->
  <meta charset="utf-8">
  <title>Sahib Dhanjal</title>
  <meta name="author" content="Sahib Dhanjal">
  <meta name="description" content=" Sahib Dhanjal - Portfolio">
  <meta name="keywords"
    content="portfolio, star wars, star, wars, robot, robotics, roboticist, University, of, michigan, university of michigan, ann arbor, flint, dearborn, bits, pilani, birla institute of technology and science, birla, institute, technology, science, pilani, hyderabad, goa, personal, projects, robot Kinematic, kinematics, path planner, path, planning, turtlebot, masters, football, soccer, Programmer, coder, coding, programming, machine learning, deep learning, tensor flow, computer vision, C/C++, JavaScript, java, python,Formula, student, fsae, italy, robotx, maritime, autonmous, slam, localization, mapping, vehicle, dynamics, inspired, Karters, fs">

  <!-- mobile specific metas
    ================================================== -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- CSS
    ================================================== -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"
    integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
  <link rel="stylesheet" href="css/style.min.css">

  <!-- script
    ================================================== -->
  <script src="js/pace.min.js"></script>

  <!-- favicons
    ================================================== -->
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
  <link rel="icon" href="images/favicon.ico" type="image/x-icon">


</head>

<body id="top">

  <!-- header
    ================================================== -->
  <header class="s-header">
    <div class="header-logo">
      <a class="site-logo" href="https://sahibdhanjal.github.io/">
        <h1 style="font-family: sans-serif; font-size: 3rem; color: white; font-family: domine-bold, sans-serif;">
          Sahib Dhanjal
        </h1>
      </a>
    </div>
    <div class="nav-mobile">
      <a id="nav-toggle" href="#!"><span></span></a>
    </div>
    <nav class="header-nav">
      <ul class="header-nav__list">
        <li class="current"><a href="#work">Work</a></li>
        <li><a href="#projects">Projects</a></li>
        <li><a href="#blog">Blog</a></li>
        <li><a href="https://docs.google.com/document/d/1M5QVAHXMq0l7ucO6YMMXVjFv8SsKv1MEqzcDgkjQo4A/"
            target="_blank">Resume<img
              src="https://cdn.prod.website-files.com/63dcb6e1a80e9454b630f4c4/63e0b50ea0956f4526968ef1_23-icon-external.svg"
              loading="lazy" alt="" class="icon-external"></a></li>
        <li><a href="https://www.linkedin.com/in/sahibdhanjal/" target="_blank">LinkedIn<img
              src="https://cdn.prod.website-files.com/63dcb6e1a80e9454b630f4c4/63e0b50ea0956f4526968ef1_23-icon-external.svg"
              loading="lazy" alt="" class="icon-external"></a></li>
      </ul>
    </nav>
  </header>


  <!-- home
    ================================================== -->
  <section id="home" class="s-home target-section" data-parallax="scroll" data-position-y=top>
    <div class="fancy-text" style="height: 35em; position: relative;">
      <div class="stars" style="height: 100%"></div>
      <div class="twinkling" style="height: 100%"></div>
      <svg class="fancy-text__main center">
        <symbol id="s-text">
          <text text-anchor="middle" x="50%" y="80%">Hello World</text>
        </symbol>
        <g class="g-ants">
          <use xlink:href="#s-text" class="text-copy"></use>
          <use xlink:href="#s-text" class="text-copy"></use>
          <use xlink:href="#s-text" class="text-copy"></use>
          <use xlink:href="#s-text" class="text-copy"></use>
          <use xlink:href="#s-text" class="text-copy"></use>
        </g>
      </svg>
    </div>
  </section>
  <!-- end s-home -->

  <!-- work
    ================================================== -->
  <section id="work" class="s-work target-section">
    <div class="section-label">
      <span class="dot"></span> WORK
    </div>

    <div class="work-experience" data-aos="fade-up">
      <!-- Magic Leap / Google -->
      <div class="experience-item">
        <div class="experience-company">Magic Leap | Google</div>
        <div class="experience-details">
          <div class="experience-header">
            <h3 class="experience-title">Perception Engineer</h3>
            <span class="experience-date">Oct '20 - Present</span>
          </div>
          <p class="experience-desc">Designing, implementing and scaling state-of-the-art algorithms across the
            perception stack, enabling seamless XR experiences under the <a
              href="https://www.magicleap.com/newsroom/magic-leap-and-google-partnership">Magic Leap - Google
              Partnership</a>.</p>
        </div>
      </div>

      <!-- Magna / Lyft Level 5 -->
      <div class="experience-item">
        <div class="experience-company">Magna | Lyft Level 5</div>
        <div class="experience-details">
          <div class="experience-header">
            <h3 class="experience-title">Localization & Mapping Engineer</h3>
            <span class="experience-date">Jul '19 - Oct '20</span>
          </div>
          <p class="experience-desc">Focused on deploying detection and localization pipelines on highly constrained
            hardware, while also developing the next generation of multi-layered HD maps under the <a
              href="https://www.magna.com/company/newsroom/releases-archive/release/2019/01/14/news-release---magna-highlights-lyft-partnership-milestones-at-2019-north-american-international-auto-show">Magna
              - Lyft Level 5 Partnership</a>.</p>
        </div>
      </div>
    </div>
  </section>
  <!-- end s-work -->

  <!-- projects
    ================================================== -->
  <section id='projects' class="s-projects target-section">
    <div class="section-label" data-aos="fade-up">
      <span class="dot"></span> PROJECTS
    </div>

    <div class="projects-grid">

      <!-- NASA Astronet -->
      <div class="project-card" data-aos="fade-up" data-date="Apr '18 - May '19">
        <div class="project-card__img"><img src="images/portfolio/nasa.jpg" alt="NASA Astronet"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">NASA Astronet</h3>
          <p class="project-card__cat">A Human-Centric Network of Co-Robots</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            As a part of the <a href="https://dasc-lab.github.io/">DASC Lab</a>, over the course of 1.5 years, I
            implemented and scaled multiple optimization-based control and coverage algorithms developed by <a
              href="https://scholar.google.com/citations?user=e4efxWQAAAAJ&hl=en">Will</a> and
            <a href="https://scholar.google.com/citations?user=ny1yTusAAAAJ&hl=en">Dr. Dimitra Panagou</a>,
            translating theoretical guarantees into a deployable and simulation-validated system. <a
              href="https://www.nasa.gov/directorates/spacetech/strg/ecf2016/AstroNet.html">AstroNet</a> was one such
            research initiative focused on enabling autonomous coverage and discrete navigation strategies for NASA’s
            free-flying <a href="https://www.nasa.gov/astrobee">Astrobee</a> robots operating aboard the ISS, with the
            goal of reducing reliance on human teleoperation.
          </p>

          <!-- Embed Video here instead of image -->
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/hkFtmTng5Wo?si=HYrI0QJOKNnxbLEI" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>

          <p>
            As can be seen in the embedded video, in this project I primarily focused on building a high-fidelity ISS
            simulation environment using <a href="http://wiki.ros.org/">ROS</a> and <a
              href="https://gazebosim.org/home">Gazebo</a> to evaluate coverage strategies under realistic microgravity
            dynamics and sensing constraints. This included developing an optimized real-time control pipeline capable
            of executing discrete coverage policies within the ROS ecosystem while maintaining stable closed-loop
            behavior.
          </p>

          <p>
            To support end-to-end evaluation, I designed a distributed architecture integrating MATLAB (core control and
            coverage algorithms), ROS (middleware), Gazebo (physics simulation), and an immersive VR visualization
            interface via an <a href="https://www.oculus.com/rift/">Oculus Rift</a>. This framework enabled closed-loop
            validation of guidance, navigation, and control (GNC) strategies prior to hardware deployment, significantly
            improving experimental throughput and reproducibility.
          </p>

          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/NASA-Astrobee" target="_blank">Github</a>
          </div>
        </div>
      </div>

      <!-- Swarm Mapping -->
      <div class="project-card" data-aos="fade-up" data-date="Dec '18 - May '19">
        <div class="project-card__img"><img src="images/portfolio/swarm.jpg" alt="Swarm Mapping"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Swarm Mapping and Control</h3>
          <p class="project-card__cat">Aerial and Ground Robot Swarm</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>This was one of the more interesting robotics projects I worked on, in collaboration with <a
              href="https://tardec.army.mil/">TARDEC</a>, and guided by <a
              href="http://www-personal.umich.edu/~dpanagou/">Dr. Dimitra Panagou</a>, where I used a mix of <a
              href="https://www.bitcraze.io/crazyflie-2/">CrazyFlie drones</a> and <a
              href="https://store.aionrobotics.com/products/r1-ardupilot">Aion R1 rovers</a> for autonomous
            reconnaissance missions using multiple robots. The goal was to get a swarm of heterogeneous robots to work
            together seamlessly—even in unpredictable environments—while avoiding collisions and keeping track of their
            positions.
          </p>

          <!-- Embed Video here instead of CrazyFly Image -->
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/TLIx-w4eX3c?si=K54d36zy_Adxo5EI" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>

          <p>
            My main focus was on multi-robot localization and collision avoidance, helping the drones and rovers
            navigate in sync without stepping on each other’s toes. I was also solely responsible for the system
            architecture as well, where I developed a pub-sub architecture using ROS for this swarm of robots to
            communicate with each other.The project gave me hands-on experience with distributed algorithms, sensor
            fusion, and resilient swarm coordination. As you can see from the videos attached, we were trying to make
            the robots follow generic routines such as following a set path or creating a defined shape while the
            control algorithms were running in real-time. From the embedded videos, you can see how a set of 4 drones,
            as well as 3 rovers were able to create basic shapes without colliding or running into each other. For the
            purposes of the shown experiments, localization was done using simple vicon motion-capture systems in indoor
            environments but using SLAM outdoors using the sensors (lidar + odometry) the rovers were equipped with.
          </p>

          <!-- Embed Video here instead of Rover Image -->
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/EFuOMRC9ebY?si=9T9KmCBpLbrL5d_J" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>

        </div>
      </div>

      <!-- Visual-Radio-Inertial Localization -->
      <div class="project-card" data-aos="fade-up" data-date="Aug '18 - May '19">
        <div class="project-card__img"><img src="images/portfolio/wifi.jpg" alt="Radio Localization"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Visual-Radio-Inertial Localization</h3>
          <p class="project-card__cat">Indoor Positioning System based on WiFi</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This project was more attuned to be a masters theses than anything. From the time I learned about SLAM, I
            wanted to see if robots/devices can locate inside indoor environments using signals from low cost sensors
            such as WiFi/BLE Beacons which are already installed in most buildings. Working under <a
              href="https://scholar.google.com/citations?user=l2jdSb8AAAAJ&hl=en">Dr. Maani Ghaffari</a>, I was working
            on exploring how radio waves, specifically WiFi can be used to accurately locate the 3DOF position of a
            device inside a GPS denied environment. After much exploration, I was able to create a deep neural network
            which was able to differentiate between Line-Of-Sight/Non-Line-Of-Sight packets with an accuracy of ~95%.
            Without this classifier, the robot often lost track of its position in the environment as seen in the
            illustration below (FastSLAM without classifier).
          </p>
          <img class="project-card__gallery-img"
            src="https://github.com/sahibdhanjal/DeepLocNet/raw/matlab/assets/FSwoC.gif" alt="Radio Localization">
          <p>
            The algorithm was initially developed and tested using the open-source <a
              href="https://pylayers.github.io/pylayers/">PyLayers</a> simulator. On confirmation that our core
            algorithm worked, we proceeded to test it out on the <a href="https://fetchrobotics.com/">Fetch</a> robot,
            using the Friis Free Space model and our classifier to act as a measurment in Fast SLAM, while using the
            robot's wheel odometry as its positioning model. A variant using particle filter based SLAM was also
            implemented to evaluate performance. In our experiments, we were able to achieve a remarkably
            accurate localization with a ~1.6m RMSE in 35m x 170m indoor environment. We can see how the FastSLAM
            performs when the classifier is used to inform its state in the animation below.
          </p>
          <img class="project-card__gallery-img"
            src="https://github.com/sahibdhanjal/DeepLocNet/raw/matlab/assets/FSwC.gif" alt="Radio Localization">
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/DeepLocNet" target="_blank">Code</a>
            <a href="https://arxiv.org/abs/2002.00484" target="_blank">Paper</a>
          </div>
        </div>
      </div>

      <!-- Unsupervised Learning AR -->
      <div class="project-card" data-aos="fade-up" data-date="Jul '18 - Sep '18">
        <div class="project-card__img"><img src="images/portfolio/arcontrol.jpg" alt="AR Control"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Unsupervised Learning — AR</h3>
          <p class="project-card__cat">Unsupervised Learning based on GMM</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            Another project where I was working with <a
              href="https://scholar.google.com/citations?user=e4efxWQAAAAJ&hl=en">Will</a> and
            <a href="https://scholar.google.com/citations?user=ny1yTusAAAAJ&hl=en">Dr. Dimitra Panagou</a> where we were
            using an AR glass, as well as a camera to provide the robot with assistive camera views. The vicon
            motion capture system was also used to track ground-truth head motions, which was then used to fit a
            Gaussian Mixture Model (GMM) to define the visual interest function of the user using online Expectation
            Maximization (EM). My role on this project was solely enablement - I was responsible for setting up the
            entire software stack for the project, including the AR glasses, the camera, and the vicon system as well as
            implementing the algorithm in C++ as well as optimizing the hell out of it using CUDA.
          </p>
          <!-- Embed Video here instead of Image -->
          <div class="video-container">
            <iframe src="https://player.vimeo.com/video/297490185?h=6a0638e0bf" width="640" height="360" frameborder="0"
              allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
          <p>
            You can see in the embedded video how the drone is autonomously able to navigate to the task which is of
            highest importance, but not being currently focused on by the operator. The core goal of this project was
            that lets say an operator has multiple tasks to do - task 1, task 2, task 3 (ordered by priority), if the
            operator is focusing on task 2 at the moment, the drone automatically navigates to task 1 while providing a
            visual feed on how that is unfolding. For in-depth details on this project, I'd highly encourage you to
            refer to the paper we published with are findings in it.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/AR-unsupervised-learning" target="_blank">Code</a>
            <a href="https://ieeexplore.ieee.org/document/8793587" target="_blank">Paper</a>
          </div>
        </div>
      </div>

      <!-- Balance Bot -->
      <div class="project-card" data-aos="fade-up" data-date="Sep '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/balance.jpg" alt="Balance Bot"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Autonomous Balance Bot</h3>
          <p class="project-card__cat">Self Balancing two-wheeled bot</p>
        </div>

        <div class="project-card__detail" style="display:none">
          <p>
            Another one of those fun projects from the <a href="https://rob550-docs.github.io/">UMich Curriculum</a>,
            in this project, a team of 2 people was responsible for building a self-balancing two-wheeled bot from
            scratch. We had to design everything from scratch — all the way from mechanical design, to the cascaded PID
            controller. The goal of the project was to autonomously navigate obstacle courses while maintaining its
            balance and positioning.
          </p>
          <!-- Embed Video here instead of image -->
          <div class="video-container">
            <iframe src="https://www.youtube.com/embed/HnjD2pixu-w?si=T4j4UxFzgoyLtDCH" width="640" height="360"
              frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
          </div>
          <p>
            We used a combination of wheel odometry and Vicon based motion capture to estimate the bot's current
            position and orientation. Control algorithms were developed to control the bot's balance, motion and
            orientation, as well as dictating what it needs to do. The bot was able to navigate obstacle courses while
            maintaining its balance and positioning, that too within very strict time limits. In a competition of 12
            teams, ours was the fastest and most stable!
          </p>
        </div>
      </div>

      <!-- 3D Bounding Box -->
      <div class="project-card" data-aos="fade-up" data-date="Oct '18 - Dec '18">
        <div class="project-card__img"><img src="images/portfolio/gta.jpg" alt="3D Bounding Box"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">3D Bounding Box Regression</h3>
          <p class="project-card__cat">3D Vehicle Detection</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            A more trivial project (compared to the previous ones ofcourse), this one was a part of our curriculum at
            UMich as well. We were given the GTA 10k dataset and were supposed to develop a custom neural network to
            regress the 3D bounding box of vehicles in the dataset using a simplistic bounding box detector and
            geometric constraints. As a solution, I developed a simplistic 20-layer SE-ResNet + YOLO 3D bounding box
            regression incorporating ideas from <a href="https://arxiv.org/abs/1612.00496">Mousavian et al.</a>. It was
            a Kaggle competition, where I ranked 5th out of 41 teams, achieving an accuracy of ~73% with the centroid
            MSE of ~9.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-gta.jpg" alt="3D Bounding Box">

        </div>
      </div>


      <!-- PoseNet++ -->
      <div class="project-card" data-aos="fade-up" data-date="Feb '18 - Apr '18">
        <div class="project-card__img"><img src="images/portfolio/posenet.jpg" alt="PoseNet++"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">PoseNet++</h3>
          <p class="project-card__cat">Deep Learning in SLAM</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>In this project, I developed an online deep-learning framework for automatic data labeling of mobile camera
            input using structure-from-motion (SfM). A deep learning–based structural motion algorithm generated
            high-quality pose labels from raw images, eliminating the need for manual annotation. These labeled poses
            were used to train PoseNet as a visual sensor model, while GPS and odometry data provided the action model
            for the robot. The approach integrated learning-based perception with probabilistic state estimation in a
            factor graph using <a href="https://borg.cc.gatech.edu/">GTSAM</a> and <a
              href="http://people.csail.mit.edu/kaess/isam/">iSAM</a>.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-posenet.jpg" alt="PoseNet++">

          <p><a
              href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf">PoseNet</a>
            served as the learned observation model, regressing camera poses from images, which were fused with
            motion priors from odometry and GPS in real time. The system was first validated in GTSAM simulations to
            ensure consistency and robustness before deployment on a differential drive mobile robot. In real-world
            operation, camera inputs were processed online, and PoseNet outputs were integrated with motion data to
            provide continuously refined pose estimates.</p>

          <p>By combining deep learning with classical state estimation, this framework offered a scalable, hybrid
            approach for robust localization. It demonstrated that automatically labeled camera data could train
            effective sensor models, while probabilistic factor-graph optimization ensured globally consistent pose
            estimation for mobile robotic systems.</p>
          <div class="project-card__links">
            <a href="https://posenet-mobile-robot.github.io/" target="_blank">Website</a>
            <a href="https://pdfs.semanticscholar.org/e389/cbc946b7e6a9baab781f8a2cdb3deea356dd.pdf"
              target="_blank">Paper</a>
          </div>
        </div>
      </div>

      <!-- Pedestrian Tracking -->
      <div class="project-card" data-aos="fade-up" data-date="Feb '18 - Apr '18">
        <div class="project-card__img"><img src="images/portfolio/ped.jpg" alt="Pedestrian Tracking"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Pedestrian Tracking</h3>
          <p class="project-card__cat">Mask R-CNN based Pedestrian Tracking</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p><a href="https://github.com/matterport/Mask_RCNN">Mask R-CNN</a> for pedestrian detection. Dense optical
            flow via <a href="http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf">Farneback's
              algorithm</a>, tracked with a particle filter.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-ped.jpg" alt="Pedestrian Tracking">
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/Mask-RCNN-Pedestrian-Detection" target="_blank">Code</a>
          </div>
        </div>
      </div>

      <!-- AUVSI RobotX -->
      <div class="project-card" data-aos="fade-up" data-date="Jan '18 - Oct '18">
        <div class="project-card__img"><img src="images/portfolio/wamv.jpg" alt="AUVSI RobotX"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">AUVSI RobotX</h3>
          <p class="project-card__cat">Autonomous Maritime Systems</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            The <a href="http://robotx.org/">AUVSI RobotX</a> Challenge is an international robotics competition
            focused on designing fully autonomous maritime systems capable of navigating complex marine environments
            without human intervention. For this project, I developed a robust software and perception stack to support
            high-fidelity autonomy on an unmanned surface vehicle (USV). I architected and implemented a scalable
            sensor-fusion framework that tightly integrates a spatial dual GPS/IMU module with high-resolution imaging
            from the FLIR Ladybug 3 and dense point-cloud data from a Velodyne HDL-32E, enabling accurate state
            estimation and situational awareness in real time.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-wamv.jpg" alt="AUVSI RobotX">
          <p>
            To meet competition requirements, I implemented classical robotic control algorithms alongside a
            YOLO-based marker detection system for visual identification of competition objects and task elements,
            improving the vehicle’s ability to localize and interact with its environment. I also developed advanced
            calibration pipelines for LiDAR–LiDAR and camera–LiDAR extrinsic alignment using PnP and 3D correspondences,
            based on established multi-sensor calibration techniques, which significantly enhanced the coherence of
            multi-modal data streams for perception and planning. Finally, I deployed a SLAM framework in ROS leveraging
            gmapping, costmap_2d, and AMCL libraries, enabling reliable mapping, localization, and autonomous waypoint
            navigation within the competition arena and demonstrating an end-to-end autonomy solution capable of
            perception, mapping, and decision-making under real-world constraints.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/MichiganRobotX/" target="_blank">Github</a>
          </div>
        </div>
      </div>

      <!-- SLAM -->
      <div class="project-card" data-aos="fade-up" data-date="Oct '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/slam.jpg" alt="SLAM"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Simultaneous Localization &amp; Mapping</h3>
          <p class="project-card__cat">Particle Filter based SLAM</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>Particle filter SLAM on a differential drive robot with Scanse Lidar. Occupancy grid mapping and <a
              href="http://robotfrontier.com/">Yamauchi's exploration</a> on <a
              href="https://www.raspberrypi.org/">Raspberry Pi 3</a> and <a
              href="https://beagleboard.org/black">BeagleBone Black</a>.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-slam.jpg" alt="SLAM">
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/SLAM/blob/master/BotLab%20Report.pdf" target="_blank">Code</a>
          </div>
        </div>
      </div>

      <!-- Kinematics Simulator -->
      <div class="project-card" data-aos="fade-up" data-date="Sep '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/kinsim.jpg" alt="Kinematics Simulator"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Robot Kinematics Simulator</h3>
          <p class="project-card__cat">Javascript based Kinematic Simulator</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-kinsim.jpg" alt="Kinematics Simulator">
          <p>Web-based simulator interfacing with ROS. Parses URDFs for forward/inverse kinematics, object following,
            and RRT-Connect trajectory planning. Built for <a href="https://autorob.github.io/">Autonomous Robotics</a>.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/Robot-Kinematic-Simulator" target="_blank">Code</a>
            <a href="projects/kinematic simulator/home.html" target="_blank">Demo</a>
          </div>
        </div>
      </div>

      <!-- Pendulum Simulations -->
      <div class="project-card" data-aos="fade-up" data-date="Sep '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/pend.jpg" alt="Pendulum Simulations"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Game Physics — Pendulums</h3>
          <p class="project-card__cat">Single, Double and Cart Pole Pendulums</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>PID control with Euler, Verlet, Velocity Verlet, and Runge-Kutta (RK4) integrators.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-pend.jpg" alt="Pendulum Simulations">
          <div class="project-card__links">
            <a href="projects/pendulum/pendularm1.html" target="_blank">Demo-1</a>
            <a href="projects/pendulum/pendularm2.html" target="_blank">Demo-2</a>
            <a href="projects/pendulum/cartpole.html" target="_blank">Demo-3</a>
          </div>
        </div>
      </div>


      <!-- Vision Robotic Arm -->
      <div class="project-card" data-aos="fade-up" data-date="Sep '17 - Dec '17">
        <div class="project-card__img"><img src="images/portfolio/aarm.jpg" alt="Autonomous Arm"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Vision Based Autonomous Arm</h3>
          <p class="project-card__cat">Autonomous Robotic Arm via visual cues</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>RRR:R dynamixel arm with <a href="https://www.xbox.com/en-US/xbox-one/accessories/kinect">Microsoft
              Kinect</a>. Detects colored cubes via depth maps and spline-based trajectory planning.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-aarm.jpg" alt="Autonomous Arm">
        </div>
      </div>

      <!-- Path Planning -->
      <div class="project-card" data-aos="fade-up" data-date="Dec '15 - May '16">
        <div class="project-card__img"><img src="images/portfolio/turt.jpg" alt="Path Planning"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Path Planning | Exploration</h3>
          <p class="project-card__cat">Multi-Agent Exploration using TurtleBots</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This project focused on the development and large-scale evaluation of autonomous path planning and
            multi-robot exploration algorithms using <a href="https://www.turtlebot.com/turtlebot2/">Turtlebot</a>
            platforms within the <a href="www.ros.org/">ROS</a> ecosystem. I designed and implemented a complete
            navigation stack, including global and local planning components, enabling reliable autonomous exploration
            and waypoint navigation across structured and unknown environments.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-turt.jpg" alt="Path Planning">
          <p>To support algorithmic evaluation at scale, I developed a Python-based multi-robot simulator for autonomous
            exploration and path planning, enabling rapid prototyping and benchmarking of multi-agent coordination
            strategies. The project, sponsored by the Defence Research and Development Organisation(DRDO), involved
            high-fidelity simulation of multi-robot exploration in Gazebo, bridging theoretical planning methods with
            realistic robotic constraints.</p>

          <p>I conducted a comparative analysis of classical and multi-agent exploration algorithms across more than 50
            grid maps (50×50 resolution) and validated performance experimentally using TurtleBots across 10 physical
            and simulated environments. The work provided quantitative insights into exploration efficiency,
            scalability, coordination overhead, and convergence behavior, contributing to a reproducible evaluation
            framework for decentralized and cooperative robotic navigation strategies.
          </p>
          <div class="project-card__links">
            <a href="https://github.com/sahibdhanjal/Path-Planning-Simulator" target="_blank">Github</a>
            <a href="projects/path planning simulator/index.html" target="_blank">Demo</a>
          </div>
        </div>
      </div>

      <!-- White Board Cleaner -->
      <div class="project-card" data-aos="fade-up" data-date="May '15 - Jun '15">
        <div class="project-card__img"><img src="images/portfolio/warm.jpg" alt="Whiteboard Cleaner"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Portable White Board Cleaner</h3>
          <p class="project-card__cat">Arduino-powered RR:PR manipulator</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>Portable whiteboard cleaner (RR:PR manipulator) for boards up to 4'×6'. Arduino UNO + 4 servos. Presented
            at <a href="https://upcon15.iiita.ac.in/">IEEE UPCON'15</a>.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-warm.jpg" alt="Whiteboard Cleaner">
        </div>
      </div>

      <!-- Sense -->
      <div class="project-card" data-aos="fade-up" data-date="May '19 - Jul '19">
        <div class="project-card__img"><img src="images/portfolio/sense.jpg" alt="Sense"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Sense</h3>
          <p class="project-card__cat">Android app serving sensor data via web</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>
            This was one of my side projects where I worked on repurposing my old Android phone as a mobile sensor
            platform, streaming its built-in sensors—GPS, IMU, camera, and more—over a websocket to a web server. It was
            an attempt to reuse old hardware and start experimenting with robotics, SLAM algorithms, or real-time sensor
            data without needing expensive equipment which I was generally used to.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-sense.jpg" alt="Sense">
          <p>
            Alongside the app, I also created a <a href="www.ros.org/">ROS package</a> making it easier to subscribe
            to sensor topics and integrate the phone’s data into the SLAM algorithms I was playing around with. I also
            intentionally made it modular so as to include more sensors / custom processing in the future.
          </p>
        </div>
      </div>

      <!-- Gesture Arm -->
      <div class="project-card" data-aos="fade-up" data-date="Oct '14 - Dec '14">
        <div class="project-card__img"><img src="images/portfolio/garm.jpg" alt="Gesture Arm"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Gesture Controlled Robotic Arm</h3>
          <p class="project-card__cat">Arduino and XBee based</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>Under <a href="https://scholar.google.co.in/citations?user=rwWOPY8AAAAJ&hl=en">Dr. R.K.Mittal</a>. Arduino
            UNOs, MPU 6050 IMUs, XBee wireless. Processing app for slider control. 2m workspace, 500g payload.</p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-garm.jpg" alt="Gesture Arm">
        </div>
      </div>

      <!-- Formula SAE -->
      <div class="project-card" data-aos="fade-up" data-date="Oct '12 - Dec '14">
        <div class="project-card__img"><img src="images/portfolio/fs.jpg" alt="Formula SAE"></div>
        <div class="project-card__info">
          <h3 class="project-card__title">Formula SAE Italy</h3>
          <p class="project-card__cat">Formula Prototype Competition</p>
        </div>
        <div class="project-card__detail" style="display:none">
          <p>BITS Pilani at <a href="https://www.sae.org/attend/student-events/">FSAE Italy '14</a>, ranked 14th/46 in
            design. Double wishbone push-rod suspension, bell-crank optimization, and suspension dynamics calculations.
          </p>
          <img class="project-card__gallery-img" src="images/portfolio/gallery/g-fs.jpg" alt="Formula SAE">
          <div class="project-card__links">
            <a href="https://www.facebook.com/inspiredkarters/" target="_blank">Link</a>
          </div>
        </div>
      </div>

    </div>
    <!-- end projects-grid -->

    <div id="projects-pagination"></div>

    <!-- Project Detail Overlay -->
    <div id="project-overlay" class="project-overlay">
      <div class="project-overlay__content">
        <button class="project-overlay__close" aria-label="Close">&times;</button>
        <div class="project-overlay__header">
          <div class="project-overlay__left">
            <h2 class="project-overlay__title"></h2>
            <div class="project-overlay__desc"></div>
            <div class="project-overlay__img-wrap">
              <img class="project-overlay__img" src="" alt="">
            </div>
          </div>
          <div class="project-overlay__right">
            <div class="project-overlay__meta">
              <div class="project-overlay__meta-row">
                <span class="project-overlay__meta-label">Date:</span>
                <span class="project-overlay__meta-value project-overlay__date"></span>
              </div>
              <div class="project-overlay__meta-row" id="proj-link-row">
                <span class="project-overlay__meta-label">Link:</span>
                <span class="project-overlay__meta-value project-overlay__links"></span>
              </div>
            </div>
          </div>
        </div>
      </div>

  </section>
  <!-- end s-projects -->

  <!-- blog
    ================================================== -->
  <section id="blog" class="s-blog target-section">
    <div class="section-label" data-aos="fade-up">
      <span class="dot"></span> BLOG
    </div>
    <div class="blog-container" data-aos="fade-up">
      <p class="blog-section-sub">I also run a blog where I share interesting things I learn at work, offer tips for
        beginners, and write about tech / trading topics that others might find helpful. If you love what you see,
        definitely check out my <a href="https://sahibdhanjal.medium.com/" target="_blank" rel="noopener">entire archive
          on Medium</a> !</p>
      <div id="blog-list">
        <!-- Blog items injected here by js/blog.js -->
        <p style="color:rgba(255,255,255,0.4);">Loading posts…</p>
      </div>
      <div id="blog-pagination"></div>
    </div>
  </section>
  <!-- end s-blog -->

  <!-- contact
    ================================================== -->
  <section id="contact" class="s-contact">
    <div class="col-full cl-copyright" style="margin-bottom: 1rem; margin-top: 1rem;">
      <script>document.write(new Date().getFullYear());</script> | <a href="https://sahibdhanjal.github.io/">Sahib
        Dhanjal</a>
      <ul class="contact-social">
        <li><a href="https://www.linkedin.com/in/sahibdhanjal/" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        <li><a href="https://github.com/sahibdhanjal/" target="_blank"><i class="fa fa-github"></i></a></li>
        <li><a href="https://scholar.google.com/citations?user=sYbUGw8AAAAJ" target="_blank"><i
              class="fa fa-graduation-cap"></i></a></li>
        <li><a href="https://medium.com/@sahibdhanjal" target="_blank"><i class="fa fa-medium"></i></a></li>
      </ul>
    </div>
  </section>
  <!-- end s-contact -->



  <!-- Java Script
    ================================================== -->
  <script src="js/scripts.min.js"></script>

</body>

</html>